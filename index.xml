<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Periferia Binaria</title>
    <link>https://blog.bitclvx.com/</link>
    <description>Recent content on Periferia Binaria</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.bitclvx.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Golang HTTP Handlers</title>
      <link>https://blog.bitclvx.com/post/golang-handler/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bitclvx.com/post/golang-handler/</guid>
      <description>To process HTTP requests in golang, you need a way to handle different routes, and a actual handler to process the requests. ServeMux and the Handler interface do exactly that.
Handler Interface Golang provides the http.Handler interface to respond to HTTP requests in the net/http package. This interface only requires you to implement ServeHTTP(ResponseWriter, *Request).
type Handler interface { ServeHTTP(ResponseWriter, *Request) }  In addition, the http.HandlerFunc type is an adapter to allow the use of ordinary functions as HTTP handlers.</description>
    </item>
    
    <item>
      <title>Gitlab Runners</title>
      <link>https://blog.bitclvx.com/post/gitlab-ci/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bitclvx.com/post/gitlab-ci/</guid>
      <description>Gitlab Runners are the core component to execute your automation for the Gitlab platform.
Concepts Runners are pretty simple to manage. It consists on a golang binary which is deployed in a platform which can be a vm, container, etc. A platform can have one or more Gitlab Runners. Then, a runner can have only one executor which is the one that execute your jobs. An executor can be a shell, a docker container, ssh to jump to another host, or an orchestrator like Kubernetes, VirtualBox, or AWS.</description>
    </item>
    
    <item>
      <title>Playing with the Kubernetes API</title>
      <link>https://blog.bitclvx.com/post/playing-k8s-api/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bitclvx.com/post/playing-k8s-api/</guid>
      <description>In my journey to learn the Kubernetes API, the first part is to connect to it.
You could use simple curl using credentials from $KUBECONFIG to do it, but Kubernetes provides a simpler and better way to connect to it: kubectl proxy --port=8080. This command will create a proxy between your localhost and your target kubernetes API according to your current kubernetes context.
After that you could keep using curl to just communicate to the api like curl localhost:8080/apis/apps/v1/deployments; however, you can replicate the same with kubectl using kubectl get --raw=&amp;lt;uri&amp;gt;.</description>
    </item>
    
    <item>
      <title>Cert Manager Notes</title>
      <link>https://blog.bitclvx.com/post/cert-manager/</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bitclvx.com/post/cert-manager/</guid>
      <description>As cert-manager approaches to hit v1.0 I found a few things that I need to start tracking like versioning, secrets, and other peculiarities about its functionality on kubernetes.
How it works Cert manager has different components: an issuer, a certificate request, a certificate, an ACME order and challenge.
You define an issuer with some information like ACME server, email, a private key which is your secret to communicate with the ACME server, and a set of solvers.</description>
    </item>
    
    <item>
      <title>Docker Networking and Linux Namespaces Deep Dive</title>
      <link>https://blog.bitclvx.com/post/docker-networking/</link>
      <pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bitclvx.com/post/docker-networking/</guid>
      <description>This is a write up of my docker networking demo at the Docker Orlando meetup.
Let&amp;rsquo;s begin with the diagram.
We can see in figure 1 a server which can be a physical device or a vm(cloud or not) connected to the Internet by an interface ens3 as its gateway interface - ens3 because kvm configures it by default. Then we have four Linux bridges. One is docker0 which is the default linux bridge configured when Docker is set up.</description>
    </item>
    
  </channel>
</rss>